{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure-cognitiveservices-speech --quiet\n",
    "%pip install openai==1.30.1 --quiet\n",
    "%pip install sounddevice --quiet\n",
    "%pip install azure-ai-formrecognizer azure-cognitiveservices-speech azure-identity --quiet\n",
    "%pip install load_dotenv --quiet\n",
    "%pip install prompt_toolkit --quiet\n",
    "%pip install azure-storage-blob --quiet\n",
    "%pip install matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv('.env')\n",
    "\n",
    "# Access the environment variables\n",
    "speech_key = os.getenv('SPEECH_KEY')\n",
    "speech_region = os.getenv('SPEECH_REGION')\n",
    "whisper_key = os.getenv('WHISPER_KEY')\n",
    "whisper_endpoint = os.getenv('WHISPER_ENDPOINT')\n",
    "whisper_deployment = os.getenv('WHISPER_DEPLOYMENT')\n",
    "storage_endpoint = os.getenv('AZURE_STORAGE_ENDPOINT')\n",
    "storage_sas = os.getenv('AZURE_STORAGE_SAS_TOKEN')\n",
    "\n",
    "\n",
    "# Use the environment variables in your code\n",
    "print(f'Speech API Key: {speech_key}')\n",
    "print(f'Speech Region: {speech_region}')\n",
    "print(f'Whisper API Key: {whisper_key}')\n",
    "print(f'Whisper Endpoint: {whisper_endpoint}')\n",
    "print(f'Whisper Deployment: {whisper_deployment}')\n",
    "print(f'Storage Endpoint: {storage_endpoint}')\n",
    "print(f'Storage SAS Token: {storage_sas}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azure.cognitiveservices.speech as speechsdk\n",
    "\n",
    "def recognize_from_microphone():\n",
    "    # This example requires environment variables named \"SPEECH_KEY\" and \"SPEECH_REGION\"\n",
    "    speech_config = speechsdk.SpeechConfig(subscription=speech_key, region=speech_region)\n",
    "    speech_config.speech_recognition_language=\"en-US\"\n",
    "\n",
    "    audio_config = speechsdk.audio.AudioConfig(use_default_microphone=True)\n",
    "    speech_recognizer = speechsdk.SpeechRecognizer(speech_config=speech_config, audio_config=audio_config)\n",
    "\n",
    "    print(\"Speak into your microphone.\")\n",
    "    speech_recognition_result = speech_recognizer.recognize_once_async().get()\n",
    "\n",
    "    if speech_recognition_result.reason == speechsdk.ResultReason.RecognizedSpeech:\n",
    "        #print(\"Recognized: {}\".format(speech_recognition_result.text))\n",
    "        return speech_recognition_result.text\n",
    "    elif speech_recognition_result.reason == speechsdk.ResultReason.NoMatch:\n",
    "        print(\"No speech could be recognized: {}\".format(speech_recognition_result.no_match_details))\n",
    "    elif speech_recognition_result.reason == speechsdk.ResultReason.Canceled:\n",
    "        cancellation_details = speech_recognition_result.cancellation_details\n",
    "        print(\"Speech Recognition canceled: {}\".format(cancellation_details.reason))\n",
    "        if cancellation_details.reason == speechsdk.CancellationReason.Error:\n",
    "            print(\"Error details: {}\".format(cancellation_details.error_details))\n",
    "            print(\"Did you set the speech resource key and region values?\")\n",
    "\n",
    "text = recognize_from_microphone()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    api_key=whisper_key,\n",
    "    api_version=\"2024-02-01\",\n",
    "    azure_endpoint = whisper_endpoint\n",
    ")\n",
    "\n",
    "audio_test_file = \"./helloThere.m4a\"\n",
    "\n",
    "result = client.audio.transcriptions.create(\n",
    "    file=open(audio_test_file, \"rb\"),            \n",
    "    model=whisper_deployment\n",
    ")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "  For more samples please visit https://github.com/Azure-Samples/cognitive-services-speech-sdk \n",
    "'''\n",
    "\n",
    "import azure.cognitiveservices.speech as speechsdk\n",
    "\n",
    "speech_config = speechsdk.SpeechConfig(subscription=speech_key, region=speech_region)\n",
    "# Note: the voice setting will not overwrite the voice element in input SSML.\n",
    "speech_config.speech_synthesis_voice_name = \"en-GB-RyanNeural\"\n",
    "\n",
    "text = \"Hi, this is Ryan\"\n",
    "\n",
    "def text_to_speech(text):\n",
    "  # use the default speaker as audio output.\n",
    "  speech_synthesizer = speechsdk.SpeechSynthesizer(speech_config=speech_config)\n",
    "  result = speech_synthesizer.speak_text_async(text).get()\n",
    "  # Check result\n",
    "  if result.reason == speechsdk.ResultReason.SynthesizingAudioCompleted:\n",
    "      print(\"Speech synthesized for text [{}]\".format(text))\n",
    "  elif result.reason == speechsdk.ResultReason.Canceled:\n",
    "      cancellation_details = result.cancellation_details\n",
    "      print(\"Speech synthesis canceled: {}\".format(cancellation_details.reason))\n",
    "      if cancellation_details.reason == speechsdk.CancellationReason.Error:\n",
    "          print(\"Error details: {}\".format(cancellation_details.error_details))\n",
    "\n",
    "text_to_speech(text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "\n",
    "\n",
    "# Create the BlobServiceClient object using the account URL and SAS token\n",
    "blob_service_client = BlobServiceClient(account_url=storage_endpoint, credential=storage_sas)\n",
    "blob_container_client = blob_service_client.get_container_client(\"paintings\")\n",
    "blob_list = blob_container_client.list_blobs()\n",
    "\n",
    "for blob in blob_list:\n",
    "    print(blob.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from promptflow.core import Prompty\n",
    "from IPython.display import Image\n",
    "\n",
    "image_path = storage_endpoint + \"paintings/D4_Painting.png?\" + storage_sas\n",
    "print(\"Painting: \" + image_path)\n",
    "display(Image(url=image_path))\n",
    "firstName = \"John\"\n",
    "\n",
    "def get_oai_response(firstName=None, question=\"John\", image_path=image_path, conversation_history=[]):\n",
    "    path_to_prompty = \"basic.prompty\"\n",
    "    flow = Prompty.load(path_to_prompty)\n",
    "    image = image_path\n",
    "    context = \"this is a painting\"\n",
    "    result = flow(\n",
    "        firstName = firstName,\n",
    "        context = context,\n",
    "        question = question,\n",
    "        image = image,\n",
    "        conversation_history = conversation_history\n",
    "    )\n",
    "\n",
    "    return result\n",
    "\n",
    "result = get_oai_response(firstName=firstName, question=\"What can you tell me about the painting?\", image_path=image_path)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "# URL of the image\n",
    "image_url = \"https://th.bing.com/th/id/R.842fb9a1885e50a762ef352821d9078d?rik=ykwt7kPK%2f3ngFA&riu=http%3a%2f%2fupload.wikimedia.org%2fwikipedia%2fcommons%2fe%2fe8%2fVan_Gogh_The_Olive_Trees..jpg&ehk=o8ZPcWcu3H0Vdk%2b2E5YJ63CXJSSHb3BFrzKL3UG5HOU%3d&risl=1&pid=ImgRaw&r=0\"\n",
    "\n",
    "# Display the image\n",
    "display(Image(url=image_url))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"https://th.bing.com/th/id/R.842fb9a1885e50a762ef352821d9078d?rik=ykwt7kPK%2f3ngFA&riu=http%3a%2f%2fupload.wikimedia.org%2fwikipedia%2fcommons%2fe%2fe8%2fVan_Gogh_The_Olive_Trees..jpg&ehk=o8ZPcWcu3H0Vdk%2b2E5YJ63CXJSSHb3BFrzKL3UG5HOU%3d&risl=1&pid=ImgRaw&r=0\"\n",
    "print(\"Painting: \" + image_path)\n",
    "display(Image(url=image_path))\n",
    "conversation_history = []\n",
    "def dialog():\n",
    "    text = recognize_from_microphone()\n",
    "    result = get_oai_response(firstName=\"Linda\", question=text, image_path=image_path, conversation_history=conversation_history)\n",
    "    print(conversation_history)\n",
    "    print(result)\n",
    "    conversation_history.append(f'''\n",
    "User: {text}\n",
    "Assistant: {result}\n",
    "''')\n",
    "    speech = text_to_speech(result)\n",
    "    if \"goodbye\" in text.lower():\n",
    "        return\n",
    "    dialog()\n",
    "dialog()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
